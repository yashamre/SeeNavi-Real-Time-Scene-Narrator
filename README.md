# üëÅÔ∏è SeeNavi ‚Äì Real-Time Scene Narration for the Visually Impaired

**SeeNavi** is a voice-controlled iOS app designed to assist visually impaired users by narrating their surroundings using advanced machine learning models. Powered by on-device camera access, speech recognition, OCR, and object detection, SeeNavi delivers hands-free, real-time scene understanding.

---

## üéØ Features

- üß† **Image Captioning** ‚Äì Describes scenes using BLIP (Bootstrapped Language Image Pretraining)
- üìñ **OCR (Text Recognition)** ‚Äì Reads printed or handwritten text using Apple's Vision framework
- üß≠ **Object Detection** ‚Äì Identifies objects with spatial context (e.g., "chair on the left")
- üéô **Voice Command Support** ‚Äì Responds to:
  - ‚Äúcapture scene‚Äù
  - ‚Äúread text‚Äù
  - ‚Äúdetect objects‚Äù
  - ‚Äúreplay caption‚Äù
  - ‚Äútake photo‚Äù (hands-free capture)
- üó£ **Speech Narration** ‚Äì Speaks all outputs aloud using AVSpeechSynthesizer
- üì± **Camera Integration** ‚Äì Seamless live preview with voice-triggered photo capture
- üîÅ **Continuous Voice Control** ‚Äì Automatically restarts listening after every action
- üü¢ **Real-Time Feedback** ‚Äì ‚ÄúListening...‚Äù and ‚ÄúProcessing...‚Äù indicators

---

## üß≠ User Flow ‚Äì Designed for the Visually Impaired

SeeNavi provides a fully voice-driven loop designed to minimize screen interaction:

1. **Launch the app**
2. **Speak a command** such as:
   - ‚Äúcapture scene‚Äù
   - ‚Äúread text‚Äù
   - ‚Äúdetect objects‚Äù
   - ‚Äúreplay caption‚Äù
3. App provides **audio feedback**:
   - ‚Äúüéô Listening...‚Äù
   - ‚Äú‚è≥ Processing...‚Äù
4. SeeNavi **analyzes the camera image** and **narrates** results
5. App automatically **restarts listening** for the next command

> No taps required ‚Äî designed for continuous, natural hands-free use.

---

## üì≤ Screenshots

> Upload your screenshots to a `screenshots/` folder and replace the filenames below.

| Home View | Camera | Listening Feedback |
|-----------|--------|--------------------|
| ![Main][screenshots/main-view.png](https://github.com/yashamre/SeeNavi-Real-Time-Scene-Narrator/blob/7c5b0a1d61a6c6e252e450b0ec121c661a9eb158/Screenshots/Interface.jpeg) | ![Camera](screenshots/camera-view.png) | ![Voice](screenshots/voice-feedback.png) |

---

## üõ† Technologies

- **SwiftUI** ‚Äì Declarative native UI
- **AVFoundation** ‚Äì Text-to-speech narration
- **Speech Framework** ‚Äì Live voice command recognition
- **Vision Framework** ‚Äì OCR & object detection
- **CoreML** ‚Äì For local ML integration (optional)
- **Python Flask** ‚Äì Captioning backend with BLIP model from HuggingFace

---

## üöÄ Getting Started

### Requirements

- macOS + Xcode 15+
- iOS 15+ physical device (camera/mic access required)
- Python 3.8+ for backend
- Apple Developer Account for deployment (optional)

### Setup

1. Clone this repo:
   ```bash
   git clone https://github.com/YOUR_USERNAME/SeeNavi.git
   cd SeeNavi


2. Run the Python BLIP backend:

   ```bash
   pip install flask transformers torch pillow
   python decoder.py
   ```

3. Update IP in `ImageCaptioner.swift`:

   ```swift
   let url = URL(string: "http://<your-local-ip>:5005/decode")!
   ```

4. Open the project in Xcode and run on a real iPhone.

---

## üß™ Voice Commands Summary

| Command          | Function                             |
| ---------------- | ------------------------------------ |
| "capture scene"  | Takes photo & generates caption      |
| "read text"      | Extracts printed or handwritten text |
| "detect objects" | Lists objects in view with position  |
| "replay caption" | Repeats last spoken result           |
| "take photo"     | Triggers camera hands-free           |

---

## üìÉ License

This project is licensed under the [MIT License](LICENSE).

---

## ‚ù§Ô∏è Credits

Developed by [Yash Amre](https://github.com/YashAmre)
Image captioning powered by [Salesforce BLIP](https://huggingface.co/Salesforce/blip-image-captioning-base)
Built with love for accessibility and inclusion.
